<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG" />
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG" />
  <meta property="og:url" content="URL OF THE WEBSITE" />
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/images/nvidia-logo-horz.png" />
  <meta property="og:image:width" content="1200" />
  <meta property="og:image:height" content="630" />


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/nvidia-logo-horz.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>TOKEN</title>
  <link rel="icon" type="image/x-icon" href="static/images/nvidia-logo-horz.png">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>

<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Tokenize the World into Object-level Knowledge to Address Long-tail
              Events in Autonomous Driving</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://thomasrantian.github.io/" target="_blank">Ran(Thomas) Tian</a><sup>1,2</sup>,
              </span>
              <span class="author-block">
                <a href="https://sites.google.com/site/boyilics/home" target="_blank">Boyi Li</a><sup>1,2</sup>,
              </span>
              <span class="author-block">
                <a href="https://research.nvidia.com/person/xinshuo-weng" target="_blank">Xinshuo Weng</a><sup>2</sup>,
              </span>
              <span class="author-block">
                <a href="https://research.nvidia.com/person/yuxiao-chen" target="_blank">Yuxiao Chen</a><sup>2</sup>,
              </span>
              <span class="author-block"></span>
              <a href="https://yuewang.xyz/" target="_blank">Yue Wang</a><sup>1,3</sup>,
              </span>
              <span class="author-block">
                <a href="https://www.borisivanovic.com/" target="_blank">Boris Ivanovic</a><sup>1,2</sup>,
              </span>
              <span class="author-block">
                <a href="https://web.stanford.edu/~pavone/index.html" target="_blank">Marco Pavone</a><sup>1,4</sup>
              </span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block">CoRL, 2024</span>
              <span class="eql-cntrb"><small><br><sup>1</sup>NVIDIA Research</small></span>
              <span class="eql-cntrb"><small><br><sup>2</sup>University of California, Berkeley</small></span>
              <span class="eql-cntrb"><small><br><sup>3</sup>University of Southern California </small></span>
              <span class="eql-cntrb"><small><br><sup>4</sup>Stanford University</small></span>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- Arxiv PDF link -->
                <span class="link-block">
                  <a href="https://arxiv.org/pdf/2407.00959.pdf" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span>

                <!-- Supplementary PDF link -->
                <!-- <span class="link-block">
                  <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Supplementary</span>
                  </a>
                </span> -->

                <!-- Github link -->
                <span class="link-block">
                  <a href="" target="_blank" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code (coming soon)</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <!-- <span class="link-block">
                  <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span> -->
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>


  <!-- Teaser video-->
  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <video poster="" id="tree" autoplay controls muted loop height="100%">
          <!-- Your video here -->
          <source src="static/videos/paper_intro_video.mp4" type="video/mp4">
        </video>
        <h2 class="subtitle has-text-centered">
        </h2>
      </div>
    </div>
  </section>
  <!-- End teaser video -->

  <!-- Paper abstract -->
  <section class="section hero is-light">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              The autonomous driving industry is increasingly adopting end-to-end
              learning from sensory inputs to minimize human biases in system design. Traditional end-to-end driving
              models, however, suffer from long-tail events due to rare
              or unseen inputs within their training distributions. To address this, we propose
              TOKEN, a novel Multi-Modal Large Language Model (MM-LLM) that tokenizes
              the world into object-level knowledge, enabling better utilization of LLMâ€™s reasoning capabilities to
              enhance autonomous vehicle planning in long-tail scenarios.
              TOKEN effectively alleviates data scarcity and inefficient tokenization by leveraging a traditional
              end-to-end driving model to produce condensed and semantically enriched representations of the scene,
              which are optimized for LLM planning
              compatibility through deliberate representation and reasoning alignment training
              stages. Our results demonstrate that TOKEN excels in grounding, reasoning, and
              planning capabilities, outperforming existing frameworks with a 27% reduction
              in trajectory L2 error and a 39% decrease in collision rates in long-tail scenarios. Additionally, our
              work highlights the importance of representation alignment
              and structured reasoning in sparking the common-sense reasoning capabilities of
              MM-LLMs for effective planning.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End paper abstract -->


  <!-- Image carousel -->
  <section class="hero is-small">
    <div class="hero-body">
      <div class="container">
        <div id="results-carousel" class="carousel results-carousel">
          <div class="item">
            <!-- Your image here -->
            <img src="static/images/TOKEN_front_fig.png" alt="MY ALT TEXT" />
            <h2 class="subtitle has-text-centered">
              TOKEN is a novel Multi-Modal Large Language Model (MM-LLM) that tokenizes the world into object-level
              knowledge, enabling better utilization of LLM's reasoning capabilities to enhance autonomous vehicle
              planning in long-tail scenarios.
            </h2>
          </div>
          <div class="item">
            <!-- Your image here -->
            <img src="static/images/TOKEN_e2e_model.png" alt="MY ALT TEXT" />
            <h2 class="subtitle has-text-centered">
              TOKEN obtains object-centric tokens from existing end-to-end autonomous driving stacks
              and uses a condensed and semantically-informed representation to encode the scene..
            </h2>
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End image carousel -->













  <!-- Paper poster -->
  <!-- <section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Poster</h2>

      <iframe  src="static/pdfs/sample.pdf" width="100%" height="550">
          </iframe>
        
      </div>
    </div>
  </section> -->
  <!--End paper poster -->


  <!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@inproceedings{tiantokenize,
        title={Tokenize the World into Object-level Knowledge to Address Long-tail Events in Autonomous Driving},
        author={Tian, Ran and Li, Boyi and Weng, Xinshuo and Chen, Yuxiao and Schmerling, Edward and Wang, Yue and Ivanovic, Boris and Pavone, Marco},
        booktitle={8th Annual Conference on Robot Learning}
      }</code></pre>
    </div>
  </section>
  <!--End BibTex citation -->


  <footer class="footer">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">

            <p>
              Template source: <a href="https://github.com/eliahuhorwitz/Academic-project-page-template"
                target="_blank">Academic Project Page Template</a>.
            </p>

          </div>
        </div>
      </div>
    </div>
  </footer>

  <!-- Statcounter tracking code -->

  <!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

  <!-- End of Statcounter Code -->

</body>

</html>